{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN from Scratch\n",
    "\n",
    "[Tutorial](https://github.com/gy910210/rnn-from-scratch)\n",
    "\n",
    "Focuses on BPTT, based on computation graph and do automated differentiation. \n",
    "\n",
    "Equations:\n",
    "s_t = tanh(Ux_t + Ws_(t-1))\n",
    "y^_t = softmax(Vs_t)\n",
    "\n",
    "CE loss, E_t(y_t, y^_t) = -y_t * log y^_t\n",
    "E(y, y^) = Sum_t [E_t(y_t, y^_t)] = - Sum_t [y_t log y^_t] ; Total error is sum of errors at each time step.\n",
    "\n",
    "The gradients are also summed up dE/dW = Sum_t [dE/dW] for U, V, W. dE/dV is simple. dE/dW, dE/dU requires BPTT. \n",
    "\n",
    "![bp](figures/rnn-bptt-with-gradients.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Units\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplyGate:\n",
    "    def forward(self,W, x):\n",
    "        return np.dot(W, x)\n",
    "    def backward(self, W, x, dz):\n",
    "        dW = np.asarray(np.dot(np.transpose(np.asmatrix(dz)), np.asmatrix(x)))\n",
    "        dx = np.dot(np.transpose(W), dz)\n",
    "        return dW, dx\n",
    "\n",
    "class AddGate:\n",
    "    def forward(self, x1, x2):\n",
    "        return x1 + x2\n",
    "    def backward(self, x1, x2, dz):\n",
    "        dx1 = dz * np.ones_like(x1)\n",
    "        dx2 = dz * np.ones_like(x2)\n",
    "        return dx1, dx2\n",
    "    \n",
    "class Sigmoid:\n",
    "    def forward(self, x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "    def backward(self, x, top_diff):\n",
    "        output = self.forward(x)\n",
    "        return (1.0 - output) * output * top_diff\n",
    "\n",
    "class Tanh:\n",
    "    def forward(self, x):\n",
    "        return np.tanh(x)\n",
    "    def backward(self, x, top_diff):\n",
    "        output = self.forward(x)\n",
    "        return (1.0 - np.square(output)) * top_diff\n",
    "    \n",
    "class Softmax:\n",
    "    def predict(self, x):\n",
    "        exp_scores = np.exp(x)\n",
    "        return exp_scores / np.sum(exp_scores)\n",
    "    def loss(self, x, y):\n",
    "        probs = self.predict(x)\n",
    "        return -np.log(probs[y])\n",
    "    def diff(self, x, y):\n",
    "        probs = self.predict(x)\n",
    "        probs[y] -= 1.0\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Layer\n",
    "Computation graph\n",
    "\n",
    "![cg1](figures/rnn-compuattion-graph.png)\n",
    "![cg2](figures/rnn-compuattion-graph_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mulGate = MultiplyGate()\n",
    "addGate = AddGate()\n",
    "activation = Tanh()\n",
    "\n",
    "class RNNLayer:\n",
    "    def forward(self, x, prev_s, U, W, V):\n",
    "        self.mulu = mulGate.forward(U, x)\n",
    "        self.mulw = mulGate.forward(W, prev_s)\n",
    "        self.add = addGate.forward(self.mulw, self.mulu)\n",
    "        self.s = activation.forward(self.add)\n",
    "        self.mulv = mulGate.forward(V, self.s)\n",
    "        \n",
    "    def backward(self, x, prev_s, U, W, V, diff_s, dmulv):\n",
    "        self.forward(x, prev_s, U, W, V)\n",
    "        dV, dsv = mulGate.backward(V, self.s, dmulv)\n",
    "        ds = dsv + diff_s\n",
    "        dadd = activation.backward(self.add, ds)\n",
    "        dmulw, dmulu = addGate.backward(self.mulw, self.mulu, dadd)\n",
    "        dW, dprev_s = mulGate.backward(W, prev_s, dmulw)\n",
    "        dU, dx = mulGate.backward(U, x, dmulu)\n",
    "        return (dprev_s, dU, dW, dV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Initialization\n",
    "\n",
    "Initializing U, V, W. For tanh activation, best between [-1/sqrt(n), 1/sqrt(n)] where n is number of incoming connections from previous layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime \n",
    "\n",
    "class Model:\n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        self.U = np.random.uniform(-np.sqrt(1. / word_dim), np.sqrt(1. / word_dim), (hidden_dim, word_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1. / hidden_dim), np.sqrt(1. / hidden_dim), (hidden_dim, hidden_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1. / hidden_dim), np.sqrt(1. / hidden_dim), (word_dim, hidden_dim))\n",
    "        \n",
    "    def forward_propagation(self, x):\n",
    "        \"\"\"\n",
    "        forward propagation (predicting word probabilities)\n",
    "        \n",
    "        x is one single data, and a batch of data\n",
    "        for example x = [0, 179, 341, 416], then its y = [179, 341, 416, 1]\n",
    "        \"\"\"\n",
    "        # The total number of time steps\n",
    "        T = len(x)\n",
    "        layers = []\n",
    "        prev_s = np.zeros(self.hidden_dim)\n",
    "        # For each time step...\n",
    "        for t in range(T):\n",
    "            layer = RNNLayer()\n",
    "            input = np.zeros(self.word_dim)\n",
    "            input[x[t]] = 1\n",
    "            layer.forward(input, prev_s, self.U, self.W, self.V)\n",
    "            prev_s = layer.s\n",
    "            layers.append(layer)\n",
    "        return layers\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"generates results\"\"\"\n",
    "        output = Softmax()\n",
    "        layers = self.forward_propagation(x)\n",
    "        return [np.argmax(output.predict(layer.mulv)) for layer in layers]\n",
    "    \n",
    "    def calculate_loss(self, x, y):\n",
    "        assert len(x) == len(y)\n",
    "        output = Softmax()\n",
    "        layers = self.forward_propagation(x)\n",
    "        loss = 0.0\n",
    "        for i, layer in enumerate(layers):\n",
    "            loss += output.loss(layer.mulv, y[i])\n",
    "        return loss / float(len(y))\n",
    "\n",
    "    def calculate_total_loss(self, X, Y):\n",
    "        loss = 0.0\n",
    "        for i in range(len(Y)):\n",
    "            loss += self.calculate_loss(X[i], Y[i])\n",
    "        return loss / float(len(Y))\n",
    "    \n",
    "    def bptt(self, x, y):\n",
    "        assert len(x) == len(y)\n",
    "        output = Softmax()\n",
    "        layers = self.forward_propagation(x)\n",
    "        dU = np.zeros(self.U.shape)\n",
    "        dV = np.zeros(self.V.shape)\n",
    "        dW = np.zeros(self.W.shape)\n",
    "\n",
    "        T = len(layers)\n",
    "        prev_s_t = np.zeros(self.hidden_dim)\n",
    "        diff_s = np.zeros(self.hidden_dim)\n",
    "        for t in range(0, T):\n",
    "            dmulv = output.diff(layers[t].mulv, y[t])\n",
    "            input = np.zeros(self.word_dim)\n",
    "            input[x[t]] = 1\n",
    "            dprev_s, dU_t, dW_t, dV_t = layers[t].backward(input, prev_s_t, self.U, self.W, self.V, diff_s, dmulv)\n",
    "            prev_s_t = layers[t].s\n",
    "            dmulv = np.zeros(self.word_dim)\n",
    "            for i in range(t-1, max(-1, t-self.bptt_truncate-1), -1):\n",
    "                input = np.zeros(self.word_dim)\n",
    "                input[x[i]] = 1\n",
    "                prev_s_i = np.zeros(self.hidden_dim) if i == 0 else layers[i-1].s\n",
    "                dprev_s, dU_i, dW_i, dV_i = layers[i].backward(input, prev_s_i, self.U, self.W, self.V, dprev_s, dmulv)\n",
    "                dU_t += dU_i\n",
    "                dW_t += dW_i\n",
    "            dV += dV_t\n",
    "            dU += dU_t\n",
    "            dW += dW_t\n",
    "        return (dU, dW, dV)\n",
    "    \n",
    "    def sgd_step(self, x, y, learning_rate):\n",
    "        dU, dW, dV = self.bptt(x, y)\n",
    "        self.U -= learning_rate * dU\n",
    "        self.V -= learning_rate * dV\n",
    "        self.W -= learning_rate * dW\n",
    "    \n",
    "    def train(self, X, Y, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "        num_examples_seen = 0\n",
    "        losses = []\n",
    "        for epoch in range(nepoch):\n",
    "            if (epoch % evaluate_loss_after == 0):\n",
    "                loss = self.calculate_total_loss(X, Y)\n",
    "                losses.append((num_examples_seen, loss))\n",
    "                time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                print(\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss))\n",
    "                # Adjust the learning rate if loss increases\n",
    "                if len(losses) > 1 and losses[-1][1] > losses[-2][1]:\n",
    "                    learning_rate = learning_rate * 0.5\n",
    "                    print(\"Setting learning rate to %f\" % learning_rate)\n",
    "            # For each training example...\n",
    "            for i in range(len(Y)):\n",
    "                self.sgd_step(X[i], Y[i], learning_rate)\n",
    "                num_examples_seen += 1\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file...\n",
      "Parsed 79185 sentences.\n",
      "Found 62987 unique words tokens.\n",
      "Using vocabulary size 8000.\n",
      "The least frequent word in our vocabulary is 'whitebeard' and appeared 10 times.\n",
      "\n",
      "Example sentence: 'SENTENCE_START i joined a new league this year and they have different scoring rules than i'm used to. SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '['SENTENCE_START', 'i', 'joined', 'a', 'new', 'league', 'this', 'year', 'and', 'they', 'have', 'different', 'scoring', 'rules', 'than', 'i', \"'m\", 'used', 'to', '.', 'SENTENCE_END']'\n",
      "\n",
      "X_train len:  78509\n",
      "y_train len:  78509\n",
      "X_train[0]:  [0, 6, 3508, 7, 157, 801, 26, 222, 8, 33, 21, 203, 4958, 341, 92, 6, 67, 208, 5, 2]\n",
      "y_train[0]:  [6, 3508, 7, 157, 801, 26, 222, 8, 33, 21, 203, 4958, 341, 92, 6, 67, 208, 5, 2, 1]\n",
      "x:\n",
      "SENTENCE_START what are n't you understanding about this ? !\n",
      "[0, 52, 28, 17, 10, 858, 55, 26, 35, 70]\n",
      "\n",
      "y:\n",
      "what are n't you understanding about this ? ! SENTENCE_END\n",
      "[52, 28, 17, 10, 858, 55, 26, 35, 70, 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from preprocessing import getSentenceData\n",
    "\n",
    "word_dim = 8000\n",
    "hidden_dim = 100\n",
    "X_train, y_train = getSentenceData('data/reddit-comments-2015-08.csv', word_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-01 21:20:19: Loss after num_examples_seen=0 epoch=0: 8.987506\n",
      "2024-11-01 21:21:12: Loss after num_examples_seen=100 epoch=1: 8.973097\n",
      "2024-11-01 21:22:06: Loss after num_examples_seen=200 epoch=2: 8.951300\n",
      "2024-11-01 21:22:59: Loss after num_examples_seen=300 epoch=3: 8.908875\n",
      "2024-11-01 21:23:51: Loss after num_examples_seen=400 epoch=4: 8.813015\n",
      "2024-11-01 21:24:44: Loss after num_examples_seen=500 epoch=5: 7.009613\n",
      "2024-11-01 21:25:38: Loss after num_examples_seen=600 epoch=6: 6.323634\n",
      "2024-11-01 21:26:31: Loss after num_examples_seen=700 epoch=7: 6.002962\n",
      "2024-11-01 21:27:24: Loss after num_examples_seen=800 epoch=8: 5.801748\n",
      "2024-11-01 21:28:17: Loss after num_examples_seen=900 epoch=9: 5.668518\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      2\u001b[0m rnn \u001b[38;5;241m=\u001b[39m Model(word_dim, hidden_dim)\n\u001b[0;32m----> 4\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mrnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.005\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluate_loss_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 105\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, X, Y, learning_rate, nepoch, evaluate_loss_after)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;66;03m# For each training example...\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(Y)):\n\u001b[0;32m--> 105\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msgd_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m         num_examples_seen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m losses\n",
      "Cell \u001b[0;32mIn[17], line 85\u001b[0m, in \u001b[0;36mModel.sgd_step\u001b[0;34m(self, x, y, learning_rate)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msgd_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y, learning_rate):\n\u001b[0;32m---> 85\u001b[0m     dU, dW, dV \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbptt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mU \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m dU\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mV \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m dV\n",
      "Cell \u001b[0;32mIn[17], line 76\u001b[0m, in \u001b[0;36mModel.bptt\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28minput\u001b[39m[x[i]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     75\u001b[0m prev_s_i \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m layers[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39ms\n\u001b[0;32m---> 76\u001b[0m dprev_s, dU_i, dW_i, dV_i \u001b[38;5;241m=\u001b[39m \u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_s_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdprev_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdmulv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m dU_t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m dU_i\n\u001b[1;32m     78\u001b[0m dW_t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m dW_i\n",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m, in \u001b[0;36mRNNLayer.backward\u001b[0;34m(self, x, prev_s, U, W, V, diff_s, dmulv)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, prev_s, U, W, V, diff_s, dmulv):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(x, prev_s, U, W, V)\n\u001b[0;32m---> 15\u001b[0m     dV, dsv \u001b[38;5;241m=\u001b[39m \u001b[43mmulGate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdmulv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     ds \u001b[38;5;241m=\u001b[39m dsv \u001b[38;5;241m+\u001b[39m diff_s\n\u001b[1;32m     17\u001b[0m     dadd \u001b[38;5;241m=\u001b[39m activation\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd, ds)\n",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m, in \u001b[0;36mMultiplyGate.backward\u001b[0;34m(self, W, x, dz)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, W, x, dz):\n\u001b[1;32m      5\u001b[0m     dW \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(np\u001b[38;5;241m.\u001b[39mdot(np\u001b[38;5;241m.\u001b[39mtranspose(np\u001b[38;5;241m.\u001b[39masmatrix(dz)), np\u001b[38;5;241m.\u001b[39masmatrix(x)))\n\u001b[0;32m----> 6\u001b[0m     dx \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dW, dx\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "rnn = Model(word_dim, hidden_dim)\n",
    "\n",
    "losses = rnn.train(X_train[:100], y_train[:100], learning_rate=0.005, nepoch=10, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
